{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de73163-21bd-4855-937e-31e606ff6938",
   "metadata": {},
   "source": [
    "### Attention Mechanism in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f00df57-a421-445b-add3-2a893a4ea190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 3 words embedding size = 4\n",
    "embeddings = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 0.0], # I\n",
    "    [0.0, 2.0, 0.0, 2.0], # Love\n",
    "    [1.0, 1.0, 1.0, 1.0]  # NLP\n",
    "])\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66eaac11-8752-49f5-9dfe-b63e3205aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "\n",
    "W_Q = torch.randn(d_model, d_model)\n",
    "W_K = torch.randn(d_model, d_model)\n",
    "W_V = torch.randn(d_model, d_model)\n",
    "\n",
    "Q = embeddings @ W_Q\n",
    "K = embeddings @ W_K\n",
    "V = embeddings @ W_V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091b8b47-c6c6-42cb-8d89-511dddc7c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -0.4913,  -0.5518,  -0.7672],\n",
      "        [  2.5580, -12.2371,  -3.5605],\n",
      "        [  0.7877,  -6.6704,  -2.5475]])\n"
     ]
    }
   ],
   "source": [
    "scores = Q @ K.T\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a0faf7-e6df-4c73-9909-1da0f20c10ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5195e-01, 3.4146e-01, 3.0659e-01],\n",
      "        [9.5462e-01, 5.8494e-04, 4.4793e-02],\n",
      "        [8.2460e-01, 1.9803e-02, 1.5560e-01]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "scaled_scores = scores / math.sqrt(d_model)\n",
    "attention_weights = torch.softmax(scaled_scores, dim=1)\n",
    "\n",
    "print(attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2476257a-f565-43aa-8096-82e1619f62e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5858,  0.7537, -1.2524,  1.0643],\n",
      "        [-2.4474,  0.4640, -0.2237,  2.6898],\n",
      "        [-2.3961,  0.5252, -0.3923,  2.5272]])\n"
     ]
    }
   ],
   "source": [
    "output = attention_weights @ V\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727c970-5329-434d-90df-777d344536ef",
   "metadata": {},
   "source": [
    "### Self Attention that Learn Language Pattern with Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7b44a0-c5bb-4b0a-aa1e-39c319f17339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2734e331-4864-4fba-a5ad-c4317908f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"i love nlp\",\n",
    "    \"nlp loves me\",\n",
    "    \"i love learning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd99b7-8970-4b87-9e98-17606bee182e",
   "metadata": {},
   "source": [
    "### This line is to convert word to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0031dbed-bbb4-4a2c-b158-2057c38f68dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(set(\" \".join(sentences).split()))\n",
    "word_to_idx = {w: i for i,  w in enumerate(words)}\n",
    "idx_to_word = {i: w for w , i in word_to_idx.items()}\n",
    "\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d457f765-a5dc-4b22-8a07-0b967656f37f",
   "metadata": {},
   "source": [
    "### This is to Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa3d8e5-94d5-4528-8ea4-947d859a4ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [5, 3],\n",
      "        [0, 2]])\n",
      "tensor([5, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 2\n",
    "x,y = [], []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split()\n",
    "    for i in range(len(tokens) - seq_length):\n",
    "        x.append([word_to_idx[tokens[i]],\n",
    "                  word_to_idx[tokens[i + 1]]])\n",
    "        y.append(word_to_idx[tokens[i + 2]])\n",
    "\n",
    "x = torch.tensor(x)\n",
    "y = torch.tensor(y)\n",
    "\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0afa1fa-405a-47be-aeec-4d3fa59fc5a6",
   "metadata": {},
   "source": [
    "### Self Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db76ffc-2594-4ce1-8fee-a4b7f36d795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "      super().__init__()  \n",
    "      self.Wq = nn.Linear(embed_dim, embed_dim)\n",
    "      self.Wk = nn.Linear(embed_dim, embed_dim)\n",
    "      self.Wv = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2 , -1))\n",
    "        weights = torch.softmax(scores , dim = -1)\n",
    "        out = torch.matmul(weights, V)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8b0f7-5470-46fb-91bf-3b4b72dac875",
   "metadata": {},
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb2471c-bdba-4779-bd2d-f8ef487d74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_enc = PositionalEncoding(embed_dim)\n",
    "        self.attention = SelfAttention(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = self.attention(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9bcabe-49c8-4a67-af5d-21651237e4ba",
   "metadata": {},
   "source": [
    "### Training Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4fbecbe-dc0e-4b9a-bb9c-d38fc7953881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss:1.9859\n",
      "Epoch 50, Loss:0.4626\n",
      "Epoch 100, Loss:0.4623\n",
      "Epoch 150, Loss:0.4622\n",
      "Epoch 200, Loss:0.4622\n",
      "Epoch 250, Loss:0.4622\n"
     ]
    }
   ],
   "source": [
    "model = TinyTransformer(vocab_size, embed_dim = 16)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = loss_fn(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss:{loss.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517971c1-4d0b-445b-b896-ec3ad1652e65",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44fa580-aa1d-42a4-a368-7a9569f246f7",
   "metadata": {},
   "source": [
    "### Importations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "537134c9-562d-4003-901d-8fb493763c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c54fd-b43b-4390-877e-4e17aeadc335",
   "metadata": {},
   "source": [
    "### Toy data been used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce3b9c83-46e7-4ec6-8008-9480a74d4247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'i': 0, 'learning': 1, 'love': 2, 'loves': 3, 'me': 4, 'nlp': 5}\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"i love nlp\",\n",
    "    \"nlp loves me\",\n",
    "    \"i love learning\"\n",
    "]\n",
    "\n",
    "words = sorted(set(\" \".join(sentences).split()))\n",
    "word_to_idx = {w: i for i, w in enumerate(words)}\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "vocab_size = len(words)\n",
    "\n",
    "print(\"Vocabulary:\", word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e9e82-ae1c-4ce0-9fd2-e4103e29f80c",
   "metadata": {},
   "source": [
    "### Create sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5792f8dc-f02d-41ca-99bc-88176adfc339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequences tensor([[0, 2],\n",
      "        [5, 3],\n",
      "        [0, 2]])\n",
      "Labels tensor([5, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "seq_length = 2\n",
    "x,y = [],[]\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split()\n",
    "    for i in range(len(tokens) - seq_length):\n",
    "        x.append([word_to_idx[tokens[i]], word_to_idx[tokens[i + 1]]])\n",
    "        y.append(word_to_idx[tokens[i + 2]])\n",
    "\n",
    "\n",
    "x = torch.tensor(x, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "print(\"Input Sequences\", x)\n",
    "print(\"Labels\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "019f9eb0-82b2-4438-a74a-a0258d0fb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1629ae38-0e0c-4daf-afb9-38aee10be674",
   "metadata": {},
   "source": [
    "### Module Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95a5851d-c97b-42be-849e-ce6fd8020403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.Wq = nn.Linear(embed_dim, embed_dim)\n",
    "        self.Wk = nn.Linear(embed_dim, embed_dim)\n",
    "        self.Wv = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, E = x.size()\n",
    "\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "\n",
    "        Q = Q.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, S, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Causal mask\n",
    "        mask = torch.tril(torch.ones(S, S)).to(x.device)\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        out = torch.matmul(weights, V)\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(B, S, E)\n",
    "        return self.out_proj(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "748d8348-5374-4335-bf5f-fc57a3ba5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bed6bb8a-cb12-4621-b459-3e852b645e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = FeedForward(embed_dim)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x + self.attn(x))\n",
    "        x = self.ln2(x + self.ffn(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be3324-1a94-42d6-850c-faed45477717",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dd99066-3eeb-41cf-96c4-2062d5c92911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=16, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional = PositionalEncoding(embed_dim)\n",
    "        self.block = TransformerBlock(embed_dim, num_heads)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional(x)\n",
    "        x = self.block(x)\n",
    "        x = x[:, -1, :]\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb326240-4b8a-4b9a-8b3a-75845d10a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.5980\n",
      "Epoch 50, Loss: 0.4644\n",
      "Epoch 100, Loss: 0.4632\n",
      "Epoch 150, Loss: 0.4628\n",
      "Epoch 200, Loss: 0.4626\n",
      "Epoch 250, Loss: 0.4625\n"
     ]
    }
   ],
   "source": [
    "model = TinyGPT(vocab_size)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(300):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = loss_fn(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d804e-8846-403b-af85-baf3bc739dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
